{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data parsing \n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     transcript_id  transcript_position nucleotide_sequence  dwelling_time  \\\n",
      "0  ENST00000000233                  244             AAGACCA        0.00299   \n",
      "1  ENST00000000233                  244             AAGACCA        0.00631   \n",
      "2  ENST00000000233                  244             AAGACCA        0.00465   \n",
      "3  ENST00000000233                  244             AAGACCA        0.00398   \n",
      "4  ENST00000000233                  244             AAGACCA        0.00664   \n",
      "\n",
      "   std_dev  mean_signal  dwelling_time_flank1  std_dev_flank1  \\\n",
      "0     2.06        125.0               0.01770           10.40   \n",
      "1     2.53        125.0               0.00844            4.67   \n",
      "2     3.92        109.0               0.01360           12.00   \n",
      "3     2.06        125.0               0.00830            5.01   \n",
      "4     2.92        120.0               0.00266            3.94   \n",
      "\n",
      "   mean_signal_flank1  dwelling_time_flank2  std_dev_flank2  \\\n",
      "0               122.0               0.00930           10.90   \n",
      "1               126.0               0.01030            6.30   \n",
      "2               124.0               0.00498            2.13   \n",
      "3               130.0               0.00498            3.78   \n",
      "4               129.0               0.01300            7.15   \n",
      "\n",
      "   mean_signal_flank2  \n",
      "0                84.1  \n",
      "1                80.9  \n",
      "2                79.6  \n",
      "3                80.4  \n",
      "4                82.2  \n"
     ]
    }
   ],
   "source": [
    "def parse_data(json_file):\n",
    "    # Initialize an empty list to hold the parsed data\n",
    "    parsed_data = []\n",
    "\n",
    "    # Open the file and read line by line\n",
    "    with open(json_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():  # Skip empty lines\n",
    "                try:\n",
    "                    # Load the JSON data from the current line\n",
    "                    json_data = json.loads(line)\n",
    "\n",
    "                    # Iterate through the transcripts\n",
    "                    for transcript_id, positions in json_data.items():\n",
    "                        # Iterate through each position in the transcript\n",
    "                        for pos, nucleotides in positions.items():\n",
    "                            # Convert position to integer\n",
    "                            pos = int(pos)  # Ensure 'transcript_position' is an integer\n",
    "                            \n",
    "                            # Iterate through the nucleotide combinations\n",
    "                            for nucleotide_seq, reads in nucleotides.items():\n",
    "                                # Iterate through each read and extract features\n",
    "                                for read in reads:\n",
    "                                    # Create a dictionary to store features for this read\n",
    "                                    features = {\n",
    "                                        'transcript_id': transcript_id,\n",
    "                                        'transcript_position': pos,\n",
    "                                        'nucleotide_sequence': nucleotide_seq,\n",
    "                                        'dwelling_time': read[0],\n",
    "                                        'std_dev': read[1],\n",
    "                                        'mean_signal': read[2],\n",
    "                                        'dwelling_time_flank1': read[3],\n",
    "                                        'std_dev_flank1': read[4],\n",
    "                                        'mean_signal_flank1': read[5],\n",
    "                                        'dwelling_time_flank2': read[6],\n",
    "                                        'std_dev_flank2': read[7],\n",
    "                                        'mean_signal_flank2': read[8],\n",
    "                                    }\n",
    "                                    # Append the features dictionary to the list\n",
    "                                    parsed_data.append(features)\n",
    "\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error parsing line: {e}\")\n",
    "\n",
    "    # Convert the list of parsed data into a DataFrame\n",
    "    df = pd.DataFrame(parsed_data)\n",
    "    return df  # Return the DataFrame\n",
    "\n",
    "# Main code block\n",
    "if __name__ == \"__main__\":\n",
    "    jsonfile = \"C:/uni/y4s1/DSA4262/grpproj/dataset0.json\"  # Replace with your actual JSON file path\n",
    "    parsed_df = parse_data(jsonfile)  # Get the DataFrame from parse_data\n",
    "    print(parsed_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset - Features shape: (11027106, 9), Labels shape: (11027106,)\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation - Original Dataset\n",
    "\n",
    "# Load the m6A labels from the original data.info.labelled file\n",
    "original_labels_df = pd.read_csv(\"C:/uni/y4s1/DSA4262/grpproj/data.info.labelled\", header=None, names=['gene_id', 'transcript_id', 'transcript_position', 'label'])\n",
    "\n",
    "# Ensure 'transcript_position' in original_labels_df is an integer\n",
    "original_labels_df['transcript_position'] = pd.to_numeric(original_labels_df['transcript_position'], errors='coerce')\n",
    "\n",
    "# Merge features DataFrame (parsed_df) with original labels DataFrame\n",
    "original_merged_df = pd.merge(parsed_df, original_labels_df, on=['transcript_id', 'transcript_position'], how='left')\n",
    "\n",
    "# Step 1: Define features (X) and target variable (y) for the original dataset\n",
    "y_original = original_merged_df['label']  # Binary label for m6A modifications (original)\n",
    "X_original = original_merged_df.drop(columns=['transcript_id', 'transcript_position', 'nucleotide_sequence', 'label', 'gene_id'])\n",
    "\n",
    "# Step 2: Convert numeric columns and drop non-numeric ones for the original dataset\n",
    "X_original = X_original.apply(pd.to_numeric, errors='coerce')  # Convert to numeric and set invalid parsing to NaN\n",
    "X_original = X_original.dropna()  # Drop rows with NaN values\n",
    "\n",
    "# Optionally, print shapes to verify\n",
    "print(f\"Original Dataset - Features shape: {X_original.shape}, Labels shape: {y_original.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undersampled Balanced Dataset - Features shape: (1230170, 9), Labels shape: (1230170,)\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation - Balanced Dataset (undersampled)\n",
    "\n",
    "# Load the m6A labels from the balanced_labels.csv file\n",
    "underbalanced_labels_df = pd.read_csv(\"C:/uni/y4s1/DSA4262/grpproj/underbalanced_labels.csv\")\n",
    "\n",
    "# Ensure 'transcript_position' in balanced_labels_df is an integer\n",
    "underbalanced_labels_df['transcript_position'] = pd.to_numeric(underbalanced_labels_df['transcript_position'], errors='coerce')\n",
    "\n",
    "# Merge features DataFrame (parsed_df) with balanced labels DataFrame\n",
    "underbalanced_merged_df = pd.merge(parsed_df, underbalanced_labels_df, on=['transcript_id', 'transcript_position'], how='left')\n",
    "underbalanced_merged_df = underbalanced_merged_df.dropna(subset=['label'])\n",
    "\n",
    "# Step 1: Define features (X) and target variable (y) for the balanced dataset\n",
    "y_underbalanced = underbalanced_merged_df['label']  # Binary label for m6A modifications (balanced)\n",
    "X_underbalanced = underbalanced_merged_df.drop(columns=['transcript_id', 'transcript_position', 'nucleotide_sequence', 'label', 'gene_id'])\n",
    "\n",
    "# Step 2: Convert numeric columns and drop non-numeric ones for the balanced dataset\n",
    "X_underbalanced = X_underbalanced.apply(pd.to_numeric, errors='coerce')  # Convert to numeric and set invalid parsing to NaN\n",
    "X_underbalanced = X_underbalanced.dropna()  # Drop rows with NaN values\n",
    "\n",
    "# Optionally, print shapes to verify\n",
    "print(f\"Undersampled Balanced Dataset - Features shape: {X_underbalanced.shape}, Labels shape: {y_underbalanced.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training with original labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for evaluation metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in y_test: {'0', '1'}\n",
      "Unique values in y_pred: {'0', '1'}\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_original contains the features and y_original contains the labels\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_original, y_original, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_model_original = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model_original.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model_original.predict(X_test)\n",
    "\n",
    "print(\"Unique values in y_test:\", set(y_test))\n",
    "print(\"Unique values in y_pred:\", set(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "import pickle\n",
    "with open('rf_model_original.pkl','wb') as f:\n",
    "    pickle.dump(rf_model_original,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  load model\n",
    "with open('rf_model_original.pkl', 'rb') as f:\n",
    "    rf_model_original = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9552\n",
      "Precision: 0.5606\n",
      "Recall: 0.0427\n",
      "F1 Score: 0.0794\n",
      "AUC-ROC: 0.7828\n"
     ]
    }
   ],
   "source": [
    "# Ensure predictions and test labels are integers\n",
    "y_test = np.array(y_test)\n",
    "y_test = y_test.astype(int)\n",
    "y_pred = y_pred.astype(int)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='binary', zero_division=0)  # Handle cases with no positive predictions\n",
    "recall = recall_score(y_test, y_pred, average='binary', zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, average='binary', zero_division=0)\n",
    "\n",
    "# Binarize the labels for ROC AUC calculation\n",
    "lb = LabelBinarizer()\n",
    "y_test_binarized = lb.fit_transform(y_test)\n",
    "y_pred_proba = rf_model_original.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "auc_roc = roc_auc_score(y_test_binarized, y_pred_proba)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"AUC-ROC: {auc_roc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training with balanced labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training (70%) and testing (30%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_underbalanced, y_underbalanced, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_model_under = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model_under.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model_under.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "import pickle\n",
    "with open('rf_model_under.pkl','wb') as f:\n",
    "    pickle.dump(rf_model_under,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  load model\n",
    "with open('rf_model_under.pkl', 'rb') as f:\n",
    "    rf_model_under = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7294\n",
      "Precision: 0.6913\n",
      "Recall: 0.6035\n",
      "F1 Score: 0.6444\n",
      "AUC-ROC: 0.7993\n"
     ]
    }
   ],
   "source": [
    "# Ensure predictions and test labels are integers\n",
    "y_test = np.array(y_test)\n",
    "y_test = y_test.astype(int)\n",
    "y_pred = y_pred.astype(int)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='binary', zero_division=0)  # Handle cases with no positive predictions\n",
    "recall = recall_score(y_test, y_pred, average='binary', zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, average='binary', zero_division=0)\n",
    "\n",
    "# Binarize the labels for ROC AUC calculation\n",
    "lb = LabelBinarizer()\n",
    "y_test_binarized = lb.fit_transform(y_test)\n",
    "y_pred_proba = rf_model_under.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "auc_roc = roc_auc_score(y_test_binarized, y_pred_proba)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"AUC-ROC: {auc_roc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:\n",
    "Original Dataset (0.9552): A high accuracy indicates that the model correctly predicts the majority of instances. However, this high accuracy can be misleading in genomics, particularly with imbalanced datasets where m6A modifications (positive class) may be rare compared to non-modified sites (negative class).\n",
    "Balanced Dataset (0.7294): The lower accuracy reflects a more realistic assessment, as it considers the performance of the model on both modified and unmodified sites.\n",
    "\n",
    "Precision:\n",
    "Original Dataset (0.5606): Precision indicates the proportion of true m6A modifications among all predicted modifications. A lower precision suggests that many predicted m6A sites may be false positives, leading to unreliable results in downstream applications.\n",
    "Balanced Dataset (0.6913): A higher precision means that the model is better at correctly identifying true m6A sites. This is critical in genomics, where a high rate of false positives can mislead research conclusions and experimental designs.\n",
    "\n",
    "Recall:\n",
    "Original Dataset (0.0427): The very low recall indicates that the original model fails to identify most of the actual m6A modifications. In the context of genomic research, missing m6A sites can result in significant gaps in understanding gene regulation.\n",
    "Balanced Dataset (0.6035): A significantly higher recall shows that the balanced model is much more effective in capturing m6A sites. This ability is crucial for genomic studies, as missing m6A sites could lead to incomplete analyses of gene expression and regulatory networks.\n",
    "\n",
    "F1 Score:\n",
    "Original Dataset (0.0794): The F1 score, which balances precision and recall, is low, suggesting that the model's overall effectiveness is poor in making reliable m6A predictions.\n",
    "Balanced Dataset (0.6444): A much higher F1 score indicates that the balanced model strikes a better balance between precision and recall, enhancing its utility in genomics research.\n",
    "\n",
    "AUC-ROC:\n",
    "Original Dataset (0.7828): While the AUC-ROC indicates some ability to distinguish between m6A and non-m6A sites, it doesn't reflect the model's reliability in practice due to the imbalanced nature of the data.\n",
    "Balanced Dataset (0.7993): The slight improvement in AUC-ROC reinforces that the balanced model has a better capacity to differentiate between the two classes, which is essential for accurate biological interpretations.\n",
    "\n",
    "Conclusion \n",
    "The undersampled balanced dataset provides a more reliable model for m6A predictions. It effectively captures both true m6A sites and reduces false positives, making it a valuable tool for researchers studying gene regulation and m6A modification patterns. While it shows high accuracy, its poor performance in recall and precision highlights significant shortcomings in capturing the complexities of m6A modifications. This model may mislead researchers by suggesting high confidence in predictions that may not hold true in practical applications. Overall, the balanced model's superior performance across all relevant metrics enhances its applicability in genomics research, enabling better insights into m6A modifications and their biological implications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we concluded that the balanced data set..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning (Hyperopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training (70%) and testing (30%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the objective function for Hyperopt\n",
    "def objective(params):\n",
    "    rf_model = RandomForestClassifier(**params, random_state=42)\n",
    "    \n",
    "    # Use cross-validation to evaluate model performance\n",
    "    cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='roc_auc')  # Using ROC AUC for optimization\n",
    "    mean_auc = np.mean(cv_scores)\n",
    "    \n",
    "    # Return the negative mean AUC (because Hyperopt minimizes the objective function)\n",
    "    return {'loss': -mean_auc, 'status': STATUS_OK}\n",
    "\n",
    "# Define the parameter space for Hyperopt\n",
    "space = {\n",
    "    'n_estimators': hp.choice('n_estimators', range(50, 300)),  # Number of trees in the forest\n",
    "    'max_depth': hp.choice('max_depth', range(10, 50)),         # Maximum depth of each tree\n",
    "    'min_samples_split': hp.choice('min_samples_split', range(2, 10)),  # Minimum samples to split a node\n",
    "    'min_samples_leaf': hp.choice('min_samples_leaf', range(1, 10)),    # Minimum samples at a leaf node\n",
    "    'max_features': hp.choice('max_features', ['auto', 'sqrt', 'log2']), # Number of features to consider\n",
    "    'bootstrap': hp.choice('bootstrap', [True, False])          # Whether to bootstrap samples\n",
    "}\n",
    "\n",
    "# Initialize Trials object to keep track of Hyperopt's progress\n",
    "trials = Trials()\n",
    "\n",
    "# Run the Hyperopt optimization\n",
    "best_params = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "\n",
    "# Print best hyperparameters found\n",
    "print(f\"Best hyperparameters: {best_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
